{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot using NLTK",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityadamle/Chatbot-with-NLTK/blob/main/Chatbot_using_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45AslXDoI-ss",
        "outputId": "c60a4d29-55e1-46ed-894c-edfc69ea5c1f"
      },
      "source": [
        "#import library\r\n",
        "import nltk\r\n",
        "nltk.download(\"punkt\")\r\n",
        "nltk.download(\"wordnet\")\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "import random \r\n",
        "import string\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W79PeO65JtEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "cc094e25-11d7-433c-ff06-5731619e5dbf"
      },
      "source": [
        "#Select an input text file as the bot's data \r\n",
        "file_name = \"/content/nlp.txt\"\r\n",
        "File = open(file_name, \"r\", errors = \"ignore\")\r\n",
        "text = File.read()\r\n",
        "text = text.lower()\r\n",
        "sent_tokens = nltk.sent_tokenize(text)\r\n",
        "word_tokens = nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0a714afcee26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Select an input text file as the bot's data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/nlp.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/nlp.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1e7N19nMMPm",
        "outputId": "c650b5d1-3f82-4271-c401-f62e612a7622"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural language processing\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\n\\nan automated online assistant providing customer service on a web page, an example of an application where natural language processing is a major component.',\n",
              " '[1]\\nnatural language processing (nlp) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdWCh8gMisk",
        "outputId": "e7d3a486-f393-4248-b1a1-6e74450c7740"
      },
      "source": [
        "word_tokens[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural', 'language']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt3Si8YeMsIq"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgavZKLeMnhW"
      },
      "source": [
        "wordnet = nltk.stem.WordNetLemmatizer()\r\n",
        "def lemmatize_tokens(tokens):\r\n",
        "  return [wordnet.lemmatize(token) for token in tokens] \r\n",
        "remove_punct = dict((ord(punct), None) for punct in string.punctuation)\r\n",
        "def Normalize_Ques(text):\r\n",
        "  return lemmatize_tokens(nltk.word_tokenize(text.lower().translate(remove_punct)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z11GJAAO3x_"
      },
      "source": [
        "#Handle greetings from user\r\n",
        "#input greetings\r\n",
        "GREET_INS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\r\n",
        "#select anyone randomly and respond\r\n",
        "GREET_ANS = [\"hi\", \"hey\", \"hello!\", \"hi there\",\"I am glad! You are talking to me\"]\r\n",
        "def greet(sent):\r\n",
        "  for word in sent.split():\r\n",
        "    if word.lower() in GREET_INS:\r\n",
        "      return random.choice(GREET_ANS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7CAJ8Wt56zp"
      },
      "source": [
        "#TF-idF\n",
        "\n",
        "One issue with simple countvectorizer is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors.\n",
        "\n",
        "“Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "Term Frequency: This summarizes how often a given word appears within a document.\n",
        "\n",
        "Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
        "\n",
        "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ch7wid-4QeI"
      },
      "source": [
        "#Cosine Similarity:\n",
        "Cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. In natural language terminology it is a measure of how similar the documents are which is independent of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.\n",
        "Generally, the similarity of documents is based on counting the maximum number of common words between the documents. But this approach tends to inaccuracy as the size of the document increases and the number of common words also increase even if the documents talk about different topics.\n",
        "The cosine similarity is a better alternative for ‘count-the-common-words’ or Euclidean distance approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QM3LQ6meAeA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3agU2rHeAMH"
      },
      "source": [
        "#handle user questions\r\n",
        "def response(user_response):\r\n",
        "  bot_response = \"\"\r\n",
        "  sent_tokens.append(user_response)\r\n",
        "  TfidfVec = TfidfVectorizer(tokenizer= Normalize_Ques,stop_words=\"english\")\r\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\r\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\r\n",
        "  idx = vals.argsort()[0][-2]\r\n",
        "  flat = vals.flatten()\r\n",
        "  flat.sort()\r\n",
        "  req_tfidf = flat[-2]\r\n",
        "  # If nothing matches between question and text\r\n",
        "  if(req_tfidf == 0):\r\n",
        "    bot_response = bot_response+\"I am sorry! I don't understand you\"\r\n",
        "    return bot_response\r\n",
        "  else:\r\n",
        "    bot_response = bot_response+sent_tokens[idx]\r\n",
        "    return bot_response\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS4x7TUSd_7x",
        "outputId": "20d28405-2881-4b71-d7f0-4dc392af24e5"
      },
      "source": [
        "#Built chatbot\r\n",
        "#Set flag to keep bot running\r\n",
        "flag = True\r\n",
        "#print an introduction at the beginnng\r\n",
        "print(\"Chatbot: My name is Tony. I will answer your questions about Natural Language Processing. If you want to leave then type Bye!\")\r\n",
        "while(flag == True):\r\n",
        "  user_response = input()\r\n",
        "  user_response = user_response.lower()\r\n",
        "#If user doesnot say bye then continue conversation   \r\n",
        "  if(user_response != \"bye!\"):\r\n",
        "#If user says thanks then respond and end conversation\r\n",
        "    if(user_response == \"thanks\" or user_response == \"thank you\"):\r\n",
        "      flag=False\r\n",
        "      print(\"Chatbot: I am happy to help you.\")\r\n",
        "    else:\r\n",
        "#If user says some greeting then process and respond\r\n",
        "      if(greet(user_response)!=None):\r\n",
        "        print(\"Chatbot: \"+greet(user_response))\r\n",
        "      else:\r\n",
        "#If user asks quesion then use response function to answer\r\n",
        "        print(\"Chatbot: \",end=\"\")\r\n",
        "        print(response(user_response))\r\n",
        "        sent_tokens.remove(user_response)\r\n",
        "  else:\r\n",
        "#If user says bye then respond and end conversation\r\n",
        "    flag = False\r\n",
        "    print(\"Chatbot: Bye! See you again.\")  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chatbot: My name is Tony. I will answer your questions about Natural Language Processing. If you want to leave then type Bye!\n",
            "hi\n",
            "Chatbot: hey\n",
            "how are you?\n",
            "Chatbot: I am sorry! I don't understand you\n",
            "What's up?\n",
            "Chatbot: I am sorry! I don't understand you\n",
            "What is natural language processing?\n",
            "Chatbot: starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.\n",
            "What are types of natural language processing?\n",
            "Chatbot: starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.\n",
            "What is natural language understanding?\n",
            "Chatbot: challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\n",
            "What is undestanding?\n",
            "Chatbot: I am sorry! I don't understand you\n",
            "What was Georgetown Experiment?\n",
            "Chatbot: 1950s: the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english.\n",
            "What is Statistical NLP?\n",
            "Chatbot: since the neural turn, statistical methods in nlp research have been largely replaced by neural networks.\n",
            "What are neural networks?\n",
            "Chatbot: neural networks\n",
            "further information: artificial neural network\n",
            "a major drawback of statistical methods is that they require elaborate feature engineering.\n",
            "What are common NLP tasks?\n",
            "Chatbot: common nlp tasks\n",
            "the following is a list of some of the most commonly researched tasks in natural language processing.\n",
            "thanks\n",
            "Chatbot: I am happy to help you.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}